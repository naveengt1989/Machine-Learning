import numpy as NP
def PCA(data, dims_rescaled_data=2):

    import numpy as NP
    from scipy import linalg as LA
    m, n = data.shape
    # mean center the data
    data -= data.mean(axis=0)
    print data
    # calculate the covariance matrix
    R = NP.cov(data, rowvar=False)
    #print R
    # calculate eigenvectors & eigenvalues of the covariance matrix
    # use 'eigh' rather than 'eig' since R is symmetric, 
    # the performance gain is substantial
    evals, evecs = LA.eigh(R)
    # sort eigenvalue in decreasing order
    idx = NP.argsort(evals)[::-1]
    evecs = evecs[:,idx]
    # sort eigenvectors according to same index
    evals = evals[idx]
    # select the first n eigenvectors (n is desired dimension
    # of rescaled data array, or dims_rescaled_data)
    evecs = evecs[:, :dims_rescaled_data]
    # carry out the transformation on the data using eigenvectors
    # and return the re-scaled data, eigenvalues, and eigenvectors
    
    return NP.dot(data, evecs), evals, evecs

def test_PCA(data, dims_rescaled_data=2):
    '''
    test by attempting to recover original data array from
    the eigenvectors of its covariance matrix & comparing that
    'recovered' array with the original data
    '''
    output , eigenvalues , eigenvectors = PCA(data, dims_rescaled_data=2)
    data_recovered = NP.dot( NP.dot(eigenvectors,NP.diag(eigenvalues)), NP.linalg.inv(eigenvectors) )
    X = NP.dot( output, NP.linalg.inv(eigenvectors),)
    print X
    #print data_recovered
    #assert NP.allclose(data, data_recovered)


def plot_pca(data):
    from matplotlib import pyplot as MPL
    clr1 =  '#2026B2'
    fig = MPL.figure()
    ax1 = fig.add_subplot(111)
    data_resc, data_orig = PCA(data)
    ax1.plot(data_resc[:, 0], data_resc[:, 1], '.', mfc=clr1, mec=clr1)
    MPL.show()

input = NP.array([
        [1.0,1.0],
        [2.0,2.0]
    ])

#output,D,V = PCA(input,1)
#print (output)
test_PCA(input,2);
